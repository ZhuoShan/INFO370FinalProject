{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car = pd.read_csv('cleanData/used_car_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_copy = used_car.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchForMake = ['JEEP', 'CHEVROLET','HYUNDAI','NISSAN','HONDA','BMW','VOLKSWAGEN','GMC','FORD']\n",
    "used_car_copy = used_car_copy[used_car_copy.Make.str.contains('|'.join(searchForMake))]\n",
    "searchForModel = ['GRAND', 'WRANGLER','EQUINOXFWD','SANTA','ALTIMA2.5','ACCORD','3','JETTA','SILVERADO','SIERRA','F-1504WD','SUPER']\n",
    "used_car_copy = used_car_copy[used_car_copy.Model.str.contains('|'.join(searchForModel))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_copy['Original Price'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Original Price and Vehicle type to SUV\n",
    "used_car_copy.loc[(used_car_copy.Make == 'JEEP') & (used_car_copy.Model == 'GRAND'), 'Original Price'] = 31695\n",
    "used_car_copy.loc[(used_car_copy.Make == 'JEEP') & (used_car_copy.Model == 'GRAND'), 'Vehicle Type'] = 'SUV'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'JEEP') & (used_car_copy.Model == 'WRANGLER'), 'Original Price'] = 28045\n",
    "used_car_copy.loc[(used_car_copy.Make == 'JEEP') & (used_car_copy.Model == 'WRANGLER'), 'Vehicle Type'] = 'SUV'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'CHEVROLET') & (used_car_copy.Model == 'EQUINOXFWD'), 'Original Price'] = 31000\n",
    "used_car_copy.loc[(used_car_copy.Make == 'CHEVROLET') & (used_car_copy.Model == 'EQUINOXFWD'), 'Vehicle Type'] = 'SUV'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'HYUNDAI') & (used_car_copy.Model == 'SANTA'), 'Original Price'] = 29800\n",
    "used_car_copy.loc[(used_car_copy.Make == 'HYUNDAI') & (used_car_copy.Model == 'SANTA'), 'Vehicle Type'] = 'SUV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Original Price and Vehicle type to Sedan\n",
    "used_car_copy.loc[(used_car_copy.Make == 'NISSAN') & (used_car_copy.Model == 'ALTIMA2.5'), 'Original Price'] = 31780\n",
    "used_car_copy.loc[(used_car_copy.Make == 'NISSAN') & (used_car_copy.Model == 'ALTIMA2.5'), 'Vehicle Type'] = 'Sedan'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'HONDA') & (used_car_copy.Model == 'ACCORD'), 'Original Price'] = 27470\n",
    "used_car_copy.loc[(used_car_copy.Make == 'HONDA') & (used_car_copy.Model == 'ACCORD'), 'Vehicle Type'] = 'Sedan'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'BMW') & (used_car_copy.Model == '3'), 'Original Price'] = 45000\n",
    "used_car_copy.loc[(used_car_copy.Make == 'BMW') & (used_car_copy.Model == '3'), 'Vehicle Type'] = 'Sedan'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'VOLKSWAGEN') & (used_car_copy.Model == 'JETTA'), 'Original Price'] = 24500\n",
    "used_car_copy.loc[(used_car_copy.Make == 'VOLKSWAGEN') & (used_car_copy.Model == 'JETTA'), 'Vehicle Type'] = 'Sedan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Original Price and Vehicle type to Truck\n",
    "used_car_copy.loc[(used_car_copy.Make == 'CHEVROLET') & (used_car_copy.Model == 'SILVERADO'), 'Original Price'] = 38200\n",
    "used_car_copy.loc[(used_car_copy.Make == 'CHEVROLET') & (used_car_copy.Model == 'SILVERADO'), 'Vehicle Type'] = 'Truck'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'GMC') & (used_car_copy.Model == 'SIERRA'), 'Original Price'] = 35900\n",
    "used_car_copy.loc[(used_car_copy.Make == 'GMC') & (used_car_copy.Model == 'SIERRA'), 'Vehicle Type'] = 'Truck'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'FORD') & (used_car_copy.Model == 'F-1504WD'), 'Original Price'] = 34695\n",
    "used_car_copy.loc[(used_car_copy.Make == 'FORD') & (used_car_copy.Model == 'F-1504WD'), 'Vehicle Type'] = 'Truck'\n",
    "used_car_copy.loc[(used_car_copy.Make == 'FORD') & (used_car_copy.Model == 'SUPER'), 'Original Price'] = 33150\n",
    "used_car_copy.loc[(used_car_copy.Make == 'FORD') & (used_car_copy.Model == 'SUPER'), 'Vehicle Type'] = 'Truck'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_copy=used_car_copy.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdp = pd.read_csv('cleanData/MAGDP2_2001_2017_ALL_AREAS.csv')\n",
    "#gdp = gdp[gdp['GeoName'].str.contains('CA', na = False)]\n",
    "gdp = gdp[gdp['Description'].str.contains('All industry total', na=False)].drop('GeoFIPS', axis=1).drop('Region', axis=1).drop('TableName', axis=1).drop('ComponentName', axis=1).drop('Unit', axis=1).drop('IndustryId', axis=1).drop('IndustryClassification', axis=1).drop('Description', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zyj97\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "#Select car in top Ten most popular cities rank by sales\n",
    "\n",
    "Houston = used_car_copy[used_car_copy['City'].str.contains('Houston')]\n",
    "San_Antonio = used_car_copy[used_car_copy['City'].str.contains('San Antonio')]    \n",
    "Louisville = used_car_copy[used_car_copy['City'].str.contains('Louisville')] \n",
    "Jacksonville = used_car_copy[used_car_copy['City'].str.contains('Jacksonville')] \n",
    "Austin = used_car_copy[used_car_copy['City'].str.contains('Austin')]                          \n",
    "Columbia = used_car_copy[used_car_copy['City'].str.contains('Columbia')]                  \n",
    "Raleigh = used_car_copy[used_car_copy['City'].str.contains('Raleigh')]                             \n",
    "Orlando = used_car_copy[used_car_copy['City'].str.contains('Orlando')]   \n",
    "Colorado = used_car_copy[used_car_copy['City'].str.contains('Colorado')]       \n",
    "Philadelphia = used_car_copy[used_car_copy['City'].str.contains('Philadelphia')]    \n",
    "San_Francisco = used_car_copy[used_car_copy['City'].str.contains('San Francisco')]    \n",
    "\n",
    "\n",
    "Houston.drop(Houston.columns[0],axis=1,inplace=True)\n",
    "San_Antonio.drop(San_Antonio.columns[0],axis=1,inplace=True)\n",
    "Louisville.drop(Louisville.columns[0],axis=1,inplace=True)\n",
    "Jacksonville.drop(Jacksonville.columns[0],axis=1,inplace=True)\n",
    "Austin.drop(Austin.columns[0],axis=1,inplace=True)\n",
    "Columbia.drop(Columbia.columns[0],axis=1,inplace=True)\n",
    "Raleigh.drop(Raleigh.columns[0],axis=1,inplace=True)\n",
    "Orlando.drop(Orlando.columns[0],axis=1,inplace=True)\n",
    "Colorado.drop(Colorado.columns[0],axis=1,inplace=True)\n",
    "Philadelphia.drop(Philadelphia.columns[0],axis=1,inplace=True)\n",
    "San_Francisco.drop(San_Francisco.columns[0],axis=1,inplace=True)\n",
    "Top_cities = pd.concat([Houston, San_Antonio, Louisville, Jacksonville, Austin, Columbia, Raleigh, Orlando, Colorado, Philadelphia, San_Francisco])\n",
    "\n",
    "    \n",
    "#CAUsedCar = used_car_copy[used_car_copy['State'].str.contains('CA')]\n",
    "\n",
    "#WAUsedCar.drop(WAUsedCar.columns[0],axis=1,inplace=True)\n",
    "#CAUsedCar.drop(CAUsedCar.columns[0],axis=1,inplace=True)\n",
    "\n",
    "#WAUsedCar['City'] = WAUsedCar['City'].str.upper()\n",
    "#WAUsedCarGrouped = WAUsedCar.groupby('City')\n",
    "#WAUsedCarGrouped.size().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove cars are not made in the year of 2001 to 2017\n",
    "#CAUsedCar = CAUsedCar[(CAUsedCar['Year'] >2000) & (CAUsedCar['Year'] <2018)]\n",
    "\n",
    "#Top_cities = Top_cities[(Top_cities['Year'] >2000) & (Top_cities['Year'] <2018)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column of service time \n",
    "Top_cities['serviceTime'] = 2018 - Top_cities['Year']\n",
    "#get depreciationRate and then drop price and original price\n",
    "Top_cities['DepreciationRate'] = Top_cities['Price']/Top_cities['Original Price']\n",
    "#CAUsedCar.drop(CAUsedCar.columns[0],axis=1,inplace=True)\n",
    "#CAUsedCar.drop(CAUsedCar.columns[6],axis=1,inplace=True)\n",
    "#drop make and model column\n",
    "#CAUsedCar.drop(CAUsedCar.columns[4],axis=1,inplace=True)\n",
    "#CAUsedCar.drop(CAUsedCar.columns[4],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAUsedCar = CAUsedCar.drop(CAUsedCar[CAUsedCar.DepreciationRate > 1.0].index)\n",
    "Top_cities = Top_cities.drop(Top_cities[Top_cities.DepreciationRate > 1.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_cities['Gdp'] = 0\n",
    "Top_cities['City'] = Top_cities['City'].str.upper()\n",
    "gdp['GeoName'] = gdp['GeoName'].str.upper()\n",
    "for index,car in Top_cities.iterrows():\n",
    "    allGdp = gdp[(gdp['GeoName'].str.contains(car['City']))]\n",
    "    #col = [col for col in allGdp.columns if str(car['Year']) in col]\n",
    "    Gdp = allGdp['2017'].values.tolist()\n",
    "    if len(Gdp) > 0:\n",
    "        car['Gdp'] = Gdp[0]\n",
    "        Top_cities.at[index,'Gdp'] = float(Gdp[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City\n",
       "Plano                 50\n",
       "Charlotte             46\n",
       "Houston               40\n",
       "Raleigh               39\n",
       "San Antonio           39\n",
       "Austin                38\n",
       "Orlando               36\n",
       "Miami                 32\n",
       "Marietta              32\n",
       "Carrollton            32\n",
       "Pembroke Pines        31\n",
       "Great Neck            30\n",
       "Concord               29\n",
       "North Hollywood       28\n",
       "San Rafael            28\n",
       "Louisville            28\n",
       "Virginia Beach        25\n",
       "Portland              25\n",
       "Sacramento            24\n",
       "Arlington             23\n",
       "Marlow Heights        23\n",
       "Bellevue              23\n",
       "Huntington Station    21\n",
       "Alexandria            21\n",
       "Torrance              21\n",
       "Roseville             21\n",
       "Rochelle Park         21\n",
       "Irvine                20\n",
       "Springfield           20\n",
       "Costa Mesa            19\n",
       "                      ..\n",
       "Pharr                  1\n",
       "Perth Amboy            1\n",
       "Perrysburg             1\n",
       "Paterson               1\n",
       "Hialeah                1\n",
       "Patchogue              1\n",
       "Paris                  1\n",
       "Greenacres             1\n",
       "Greatneck              1\n",
       "Rahway                 1\n",
       "Pomona                 1\n",
       "Radcliff               1\n",
       "Ft. Worth              1\n",
       "Queens Village         1\n",
       "GILBERT                1\n",
       "Punta Gorda            1\n",
       "Prince Frederick       1\n",
       "Gardena                1\n",
       "Port Orchard           1\n",
       "Glen Allen             1\n",
       "Plainville             1\n",
       "Glen Burnie            1\n",
       "Glendale               1\n",
       "Glenview               1\n",
       "Playa Vista            1\n",
       "Plattsmouth            1\n",
       "Goldens Bridge         1\n",
       "Granite City           1\n",
       "Plantation             1\n",
       "marietta               1\n",
       "Length: 747, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_car_copy.groupby(['City']).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '8,580,015'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-ac59aa0bc52d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpopulation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cityPopulation.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"Population\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._apply_converter\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '8,580,015'"
     ]
    }
   ],
   "source": [
    "population = pd.read_csv('cityPopulation.csv', stringAsFactor = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = population[['Name','Population']][:10]\n",
    "population['Name'] = population['Name'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = pd.merge(Top_cities, population, left_on='City', right_on='Name').drop('Name', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_csv() got an unexpected keyword argument 'stringAsFactor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-bccca9fdb3a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinalData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'finalData.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstringAsFactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: to_csv() got an unexpected keyword argument 'stringAsFactor'"
     ]
    }
   ],
   "source": [
    "finalData.to_csv('finalData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"2,340,814\" at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"2,340,814\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-bddb55502eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinalData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Population'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinalData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Population\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\tools\\numeric.py\u001b[0m in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mcoerce_numeric\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             values = lib.maybe_convert_numeric(values, set(),\n\u001b[1;32m--> 133\u001b[1;33m                                                coerce_numeric=coerce_numeric)\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"2,340,814\" at position 0"
     ]
    }
   ],
   "source": [
    "finalData['Population'] = pd.to_numeric(finalData[\"Population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
